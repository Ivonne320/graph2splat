 # for Scan3R data
data :
  name            : Scan3R
  rescan: True
  temporal: False
  resplit: True
  img:
    img_step: 1
    w: 960
    h: 540
  img_encoding:
    # image patch config
    resize_w: 1024 # resize image for backbone GCVit
    resize_h: 576
    img_rotate: True # rotate w,h for backbone GCVit
    patch_w: 16 # number of patchs in width
    patch_h: 9
    record_feature: False # record feature or raw image
    use_feature: False # use feature or raw image
    preload_feature: False
    feature_dir: "Features2D/DinoV2_16_9_scan" # relative to data_root_dir/files
  scene_graph:
    obj_img_patch: "Features3D/obj_dinov2_top10_l3"
    obj_topk: 10
    obj_patch_num: 100
  preprocess :
    pc_resolutions      : [64, 128, 256, 512]
    subscenes_per_scene : 1
    filter_segment_size : 512
    min_obj_points      : 50
    anchor_type_name    : ''


# for scene graph embedding
autoencoder:
  guidance:             False
  encoder:
    rel_dim             : 41
    use_splat           : False
    attr_dim            : 164
    img_patch_feat_dim  : 1536
    img_emb_dim: 256
    multi_view_aggregator: 'transformer'
    use_pretrained : False
    # pretrained: ""
    label_file_name : labels.instances.annotated.v2.ply
    pred_subfix     : inseg.ply
    modules       : ['voxel']
    use_predicted : False
    registration  : False
    scan_type     : 'scan'
    img_transformer: False
    use_pos_enc: True

    # backbone, currently not used here because we use pre-computed features for speed up
    backbone:
      # to be read in code from env varia, relative to VLSG_SPACE
      use_pretrained: False
      cfg_file: "./src/models/GCVit/configs/gcvit/cascade_mask_rcnn_gcvit_tiny_3x_coco.py"
      pretrained: "./checkpoint/GCVit/gcvit_1k_tiny.pth.tar"
      num_reduce: 0 # should be log2(resize_w/(32*patch_w))
      backbone_dim: 1536 # dim of DinoV2 features

    # patch feature encoder
    patch:
      hidden_dims: [512] # last is the out dim
      encoder_dim: 400
      gcn_layers: 2

    other:
      drop: 0.1

    # 3D obj embedding encoder
    obj:
      # embedding_dim: 100 # fixed
      embedding_dim: 300
      embedding_hidden_dims: [656, 656]
      encoder_dim: 656

    voxel:
      in_feature_dim: 9
      out_feature_dim: 9
      in_channel_res: 64
      out_channel_res: 32
      channels: [16, 32, 64]

  decoder:
    net: "sparsestructuretrellis"
    predict_eps: False
    heads: 32
    init_scale: 0.25
    input_channels: 14 # todo(gaia): change this to the number of params of gaussian splats
    layers: 4
    n_ctx: 2048
    output_channels: 14  # todo(gaia): change this to the number of params of gaussian splats
    time_token_cond: True
    width: 1024
    latent_dim: 656

    # The following parameters are used for the upsampler in Point-e
    mean_type: epsilon
    schedule: cosine
    timesteps: 200 # 1024
    guidance_scale: 3.0
    sigma_min: 0.001
    sigma_max: 120
    s_churn: 3
    use_karras: True

# implemetation
mode: "train"
local_rank: -1

# for training
train:
  train_decoder: True
  freeze_encoder: False
  pc_res: 512
  aux_channels: 0
  batch_size: 1
  num_workers: 1
  use_pretrained: False
  optim:
    lr: 0.0005
    scheduler: "linear"
    lr_decay: 0.97
    lr_decay_steps: 1
    lr_min: 0.00005
    T_max: 1000
    T_mult: 1
    weight_decay: 0.01
    max_epoch: 10500000
    free_backbone_epoch: 10500 # freeze backbone until certain epochs
    grad_acc_steps: 1
  val_steps: 1
  visualize_steps: 1
  snapshot_steps: 1
  loss:
    use_temporal: True
    loss_type: "IntraContrastiveLoss"
    decoder_weight: 1.0
    encoder_weight: 1.0
    rot_weight: 0.
    scale_weight: 0.
    opacity_weight: 0.
    feature_weight: 0.
    photometric_weight: 0.
    alpha: 0.5
    temperature: 0.1
    margin: 0.5
    text_sds_loss_weight: 0.
    # Add sgaligner loss
  data_aug:
    use_aug: False
    img:
      rotation: 60.0
      horizontal_flip: 0.5
      vertical_flip: 0.5
      color: 0.3
    use_aug_3D: False
    pcs:
      granularity: [0.05, 0.2, 0.4]
      magnitude: [0.2, 0.4, 0.4]

  # for vis
  use_vis: False
  vis_epoch_steps: 100

# for validation
val:
  batch_size: 1
  num_workers: 2
  pretrained: "./"
